
%NQSV(INFO): ------- Output from job:0000 -------
[fnode17:13517] MCW rank 12 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode17:13517] MCW rank 13 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
[fnode20:58755] MCW rank 0 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode20:58755] MCW rank 1 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
[fnode19:22421] MCW rank 4 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode19:22421] MCW rank 5 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
[fnode21:65271] MCW rank 14 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode18:117866] MCW rank 8 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode21:65271] MCW rank 15 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
[fnode18:117866] MCW rank 9 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
[fnode22:43041] MCW rank 10 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode22:43041] MCW rank 11 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
[fnode24:115641] MCW rank 2 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode24:115641] MCW rank 3 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
[fnode23:34437] MCW rank 6 bound to socket 0[core 0[hwt 0]], socket 0[core 1[hwt 0]], socket 0[core 2[hwt 0]], socket 0[core 3[hwt 0]], socket 0[core 4[hwt 0]], socket 0[core 5[hwt 0]], socket 0[core 6[hwt 0]], socket 0[core 7[hwt 0]], socket 0[core 8[hwt 0]], socket 0[core 9[hwt 0]], socket 0[core 10[hwt 0]], socket 0[core 11[hwt 0]]: [B/B/B/B/B/B/B/B/B/B/B/B][./././././././././././.]
[fnode23:34437] MCW rank 7 bound to socket 1[core 12[hwt 0]], socket 1[core 13[hwt 0]], socket 1[core 14[hwt 0]], socket 1[core 15[hwt 0]], socket 1[core 16[hwt 0]], socket 1[core 17[hwt 0]], socket 1[core 18[hwt 0]], socket 1[core 19[hwt 0]], socket 1[core 20[hwt 0]], socket 1[core 21[hwt 0]], socket 1[core 22[hwt 0]], socket 1[core 23[hwt 0]]: [./././././././././././.][B/B/B/B/B/B/B/B/B/B/B/B]
-------------------------------------------------------------
General setup:
C++ high resolution clock is used.
The clock precision seems to be 1.00000e+01ns
-------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],13] (PID 13531)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],5] (PID 22427)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],4] (PID 22426)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],12] (PID 13530)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],8] (PID 117871)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],14] (PID 65276)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],15] (PID 65277)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],6] (PID 34442)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],9] (PID 117872)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],11] (PID 43047)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],2] (PID 115646)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],0] (PID 58760)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],7] (PID 34443)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],3] (PID 115647)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],1] (PID 58761)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[57460,1],10] (PID 43046)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
Selected Platform: Intel(R) FPGA SDK for OpenCL(TM)
-------------------------------------------------------------
Selection summary:
Platform Name: Intel(R) FPGA SDK for OpenCL(TM)
Device Name:   p520_max_sg280h : BittWare Stratix 10 OpenCL platform (aclbitt_s10_pcie0)
-------------------------------------------------------------
-------------------------------------------------------------
FPGA Setup:../../../synthesis_artifacts/LINPACK_SP/520n-h-19.4.0-19.4.0-iec/hpl_torus_IEC.aocx
Prepared FPGA successfully for global Execution!
-------------------------------------------------------------
Implementation of the LINPACK benchmark proposed in the HPCC benchmark suite for FPGA.
Version: 2.4
MPI Version:  3.1
Config. Time: Fri Jan 07 09:19:32 UTC 2022
Git Commit:   b678d6d-dirty

Summary:
Block Size                    512
Communication Type            IEC
Data Type                     cl_float
Emulate                       No
Kernel File                   ../../../synthesis_artifacts/LINPACK_SP/520n-h-19.4.0-19.4.0-iec/hpl_torus_IEC.aocx
Kernel Replications           5
MPI Ranks                     16
Matrix Size                   24576
Repetitions                   10
Test Mode                     No
Device                        p520_max_sg280h : BittWare Stratix 10 OpenCL platform (aclbitt_s10_pcie0)

-------------------------------------------------------------
Start benchmark using the given configuration. Generating data...
-------------------------------------------------------------
Generation Time: 1.97335e+01 s
-------------------------------------------------------------
Execute benchmark kernel...
-------------------------------------------------------------
Torus 0,1Start! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 0,2Start! 
Torus 1,1Start! 
Torus 3,0Start! 
Torus 0,3Start! 
Torus 2,0Start! 
Torus 3,1Start! 
Torus 1,2Start! 
Torus 2,2Start! 
Torus 3,2Start! 
Torus 3,3Start! 
Torus 2,1Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 0,1End! 
Torus 0,0End! 
Torus 1,0End! 
Torus 0,2End! 
Torus 1,2End! 
Torus 2,0End! 
Torus 2,1End! 
Torus 1,1End! 
Torus 3,0End! 
Torus 3,1End! 
Torus 0,3End! 
Torus 2,2End! 
Torus 1,3End! 
Torus 2,3End! 
Torus 3,2End! 
Torus 3,3End! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 0,2Start! 
Torus 3,1Start! 
Torus 0,1Start! 
Torus 1,1Start! 
Torus 2,0Start! 
Torus 3,3Start! 
Torus 0,3Start! 
Torus 3,0Start! 
Torus 3,2Start! 
Torus 1,2Start! 
Torus 2,1Start! 
Torus 2,2Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 0,2End! 
Torus 2,0End! 
Torus 2,1End! 
Torus 1,1End! 
Torus 1,2End! 
Torus 3,1End! 
Torus 3,0End! 
Torus 1,3End! 
Torus 0,3End! 
Torus 3,2End! 
Torus 2,2End! 
Torus 2,3End! 
Torus 3,3End! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 0,2Start! 
Torus 3,1Start! 
Torus 3,3Start! 
Torus 3,2Start! 
Torus 0,1Start! 
Torus 1,1Start! 
Torus 2,0Start! 
Torus 2,2Start! 
Torus 0,3Start! 
Torus 3,0Start! 
Torus 1,2Start! 
Torus 2,1Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 2,0End! 
Torus 1,1End! 
Torus 1,2End! 
Torus 0,2End! 
Torus 2,1End! 
Torus 3,1End! 
Torus 3,0End! 
Torus 1,3End! 
Torus 0,3End! 
Torus 3,2End! 
Torus 2,2End! 
Torus 2,3End! 
Torus 3,3End! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 0,2Start! 
Torus 3,1Start! 
Torus 3,2Start! 
Torus 3,3Start! 
Torus 0,1Start! 
Torus 1,1Start! 
Torus 2,0Start! 
Torus 3,0Start! 
Torus 0,3Start! 
Torus 2,2Start! 
Torus 1,2Start! 
Torus 2,1Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 0,2End! 
Torus 1,2End! 
Torus 2,0End! 
Torus 2,1End! 
Torus 1,1End! 
Torus 3,1End! 
Torus 3,0End! 
Torus 0,3End! 
Torus 1,3End! 
Torus 3,2End! 
Torus 2,2End! 
Torus 2,3End! 
Torus 3,3End! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 1,1Start! 
Torus 0,2Start! 
Torus 3,1Start! 
Torus 3,0Start! 
Torus 3,2Start! 
Torus 3,3Start! 
Torus 0,1Start! 
Torus 2,0Start! 
Torus 0,3Start! 
Torus 2,2Start! 
Torus 2,1Start! 
Torus 1,2Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 1,1End! 
Torus 0,2End! 
Torus 2,0End! 
Torus 2,1End! 
Torus 1,2End! 
Torus 3,1End! 
Torus 3,0End! 
Torus 1,3End! 
Torus 0,3End! 
Torus 2,2End! 
Torus 2,3End! 
Torus 3,2End! 
Torus 3,3End! 
Torus 1,1Start! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 3,0Start! 
Torus 0,2Start! 
Torus 3,1Start! 
Torus 3,2Start! 
Torus 3,3Start! 
Torus 0,1Start! 
Torus 2,0Start! 
Torus 0,3Start! 
Torus 2,2Start! 
Torus 1,2Start! 
Torus 2,1Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 0,0End! 
Torus 0,1End! 
Torus 1,0End! 
Torus 0,2End! 
Torus 2,0End! 
Torus 2,1End! 
Torus 1,2End! 
Torus 1,1End! 
Torus 3,0End! 
Torus 3,1End! 
Torus 2,2End! 
Torus 0,3End! 
Torus 2,3End! 
Torus 3,2End! 
Torus 1,3End! 
Torus 3,3End! 
Torus 1,1Start! 
Torus 1,0Start! 
Torus 3,0Start! 
Torus 0,0Start! 
Torus 3,2Start! 
Torus 0,2Start! 
Torus 3,1Start! 
Torus 3,3Start! 
Torus 0,1Start! 
Torus 0,3Start! 
Torus 1,2Start! 
Torus 2,0Start! 
Torus 2,2Start! 
Torus 2,1Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 2,0End! 
Torus 0,2End! 
Torus 2,1End! 
Torus 1,2End! 
Torus 1,1End! 
Torus 3,0End! 
Torus 3,1End! 
Torus 2,2End! 
Torus 3,2End! 
Torus 2,3End! 
Torus 0,3End! 
Torus 1,3End! 
Torus 3,3End! 
Torus 0,0Start! 
Torus 1,1Start! 
Torus 1,0Start! 
Torus 0,2Start! 
Torus 3,0Start! 
Torus 3,2Start! 
Torus 3,1Start! 
Torus 3,3Start! 
Torus 0,1Start! 
Torus 1,2Start! 
Torus 0,3Start! 
Torus 2,2Start! 
Torus 2,0Start! 
Torus 2,1Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 0,2End! 
Torus 1,1End! 
Torus 1,2End! 
Torus 2,0End! 
Torus 2,1End! 
Torus 3,0End! 
Torus 3,1End! 
Torus 0,3End! 
Torus 1,3End! 
Torus 3,2End! 
Torus 2,2End! 
Torus 2,3End! 
Torus 3,3End! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 1,1Start! 
Torus 0,2Start! 
Torus 3,0Start! 
Torus 3,2Start! 
Torus 3,1Start! 
Torus 2,1Start! 
Torus 3,3Start! 
Torus 1,3Start! 
Torus 2,3Start! 
Torus 0,1Start! 
Torus 1,2Start! 
Torus 0,3Start! 
Torus 2,0Start! 
Torus 2,2Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 0,2End! 
Torus 1,1End! 
Torus 2,1End! 
Torus 2,0End! 
Torus 1,2End! 
Torus 3,0End! 
Torus 3,1End! 
Torus 3,2End! 
Torus 2,3End! 
Torus 1,3End! 
Torus 2,2End! 
Torus 0,3End! 
Torus 3,3End! 
Torus 0,0Start! 
Torus 1,0Start! 
Torus 1,1Start! 
Torus 3,0Start! 
Torus 0,2Start! 
Torus 3,2Start! 
Torus 3,1Start! 
Torus 2,3Start! 
Torus 1,3Start! 
Torus 2,1Start! 
Torus 2,0Start! 
Torus 3,3Start! 
Torus 0,1Start! 
Torus 1,2Start! 
Torus 0,3Start! 
Torus 2,2Start! 
Torus 1,0End! 
Torus 0,0End! 
Torus 0,1End! 
Torus 2,1End! 
Torus 0,2End! 
Torus 2,0End! 
Torus 1,2End! 
Torus 1,1End! 
Torus 3,0End! 
Torus 3,1End! 
Torus 0,3End! 
Torus 1,3End! 
Torus 3,2End! 
Torus 2,3End! 
Torus 2,2End! 
Torus 3,3End! 
Execution Time: 6.97517e+03 s
-------------------------------------------------------------
Validate output...
-------------------------------------------------------------
  norm. resid        resid       machep   
    4.95096e-08    5.73993e-05    1.19209e-07
Validation Time: 3.90446e-02 s
         Method           best           mean         GFLOPS
          total    1.69214e+02    1.69278e+02    3.74282e+03
           GEFA    1.69214e+02    1.69278e+02    3.74271e+03
           GESL    3.12000e-07    3.93700e-07    6.19466e+07
Validation: SUCCESS!

============================================================
Request ID:             536967.nqsv
Request Name:           HPL_SP_IEC_EXE
Queue:                  fpga-grp3-b@nqsv
Number of Jobs:         8
User Name:              mellich
Group Name:             CCUSC
Created Request Time:   Mon Jan 17 19:33:20 2022
Started Request Time:   Mon Jan 17 19:33:21 2022
Ended Request Time:     Mon Jan 17 21:30:00 2022
Resources Information:
  Elapse:               6999S
  Remaining Elapse:     3801S
============================================================

%NQSV(INFO): ------- Output from job:0001 -------

%NQSV(INFO): ------- Output from job:0002 -------

%NQSV(INFO): ------- Output from job:0003 -------

%NQSV(INFO): ------- Output from job:0004 -------

%NQSV(INFO): ------- Output from job:0005 -------

%NQSV(INFO): ------- Output from job:0006 -------

%NQSV(INFO): ------- Output from job:0007 -------
